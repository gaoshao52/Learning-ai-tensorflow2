{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy 1.18.1\n",
      "pandas 0.25.3\n",
      "sklearn 0.22.1\n",
      "matplotlib 3.1.2\n",
      "tensorflow_core.python.keras.api._v2.keras 2.2.4-tf\n",
      "tensorflow 2.1.0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "for module in [np, pd, sklearn, mpl, keras, tf]:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GPU配置策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical GPU: 1\n",
      "Logical GPU: 1\n"
     ]
    }
   ],
   "source": [
    "# tf.debugging.set_log_device_placement(True)\n",
    "tf.config.set_soft_device_placement(True)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "print(\"Physical GPU: {}\".format(len(gpus)))\n",
    "\n",
    "logical_gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n",
    "print(\"Logical GPU: {}\".format(len(logical_gpus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 在一个gpu训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - ### 3.1 获取fashion mnist数据(分布式策略)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 28, 28, 1) (55000,)\n",
      "(5000, 28, 28, 1) (5000,)\n",
      "(10000, 28, 28, 1) (10000,)\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    }
   ],
   "source": [
    "# 取出fashion mnist 数据集\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(x_train_all, y_train_all), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "x_valid, x_train = x_train_all[:5000], x_train_all[5000:]\n",
    "y_valid, y_train = y_train_all[:5000], y_train_all[5000:]\n",
    "\n",
    "\n",
    "# 标准化\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "x_train_scaled = scaler.fit_transform(x_train.astype(np.float32).reshape(-1, 1)).reshape(-1, 28, 28, 1)\n",
    "x_valid_scaled = scaler.transform(x_valid.astype(np.float32).reshape(-1, 1)).reshape(-1, 28, 28, 1)\n",
    "x_test_scaled = scaler.transform(x_test.astype(np.float32).reshape(-1, 1)).reshape(-1, 28, 28, 1)\n",
    "\n",
    "print(x_train_scaled.shape, y_train.shape)\n",
    "print(x_valid_scaled.shape, y_valid.shape)\n",
    "print(x_test_scaled.shape, y_test.shape)\n",
    "\n",
    "\n",
    "# 制作dataset 数据集\n",
    "def make_dataset(images, labels, epochs, batch_size, shuffle=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(10000)\n",
    "    dataset = dataset.repeat(epochs).batch(batch_size).prefetch(50) # prefetch 先取出50个样本准备\n",
    "    return dataset\n",
    "\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# 数据加分布式，因为要把数据平均分到各个GPU\n",
    "with strategy.scope():\n",
    "    batch_size_per_replica = 256\n",
    "    batch_size = batch_size_per_replica * len(logical_gpus)\n",
    "    \n",
    "    train_dataset = make_dataset(x_train_scaled, y_train, 1, batch_size)\n",
    "    valid_dataset = make_dataset(x_valid_scaled, y_valid, 1, batch_size)\n",
    "    \n",
    "    train_dataset_distribute = strategy.experimental_distribute_dataset(train_dataset)\n",
    "    valid_dataset_distribute = strategy.experimental_distribute_dataset(valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - ### 3.2 模型定义（分布式策略）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型要加分布式\n",
    "with strategy.scope():\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Conv2D(filters=128, kernel_size=3, padding=\"same\", activation=\"relu\", input_shape=(28, 28, 1)),\n",
    "        keras.layers.Conv2D(filters=128, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
    "        keras.layers.MaxPool2D(pool_size=2),\n",
    "\n",
    "        keras.layers.Conv2D(filters=256, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
    "        keras.layers.Conv2D(filters=256, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
    "        keras.layers.MaxPool2D(pool_size=2),\n",
    "\n",
    "        keras.layers.Conv2D(filters=512, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
    "        keras.layers.Conv2D(filters=512, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
    "        keras.layers.MaxPool2D(pool_size=2),\n",
    "\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(512, activation=\"relu\"),\n",
    "        keras.layers.Dense(10, activation=\"softmax\")\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - ### 3.3 自定义训练步骤（分布式策略）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "total: 2.3035, num: 1, average: 2.3035, acc: 0.10, run_time: 2.7735INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "total: 4.6053, num: 2, average: 2.3026, acc: 0.12, run_time: 0.0573INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "total: 6.9085, num: 3, average: 2.3028, acc: 0.11, run_time: 0.0574INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "total: 9.2085, num: 4, average: 2.3021, acc: 0.12, run_time: 0.0569INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Epoch: 1, Loss:1.7983, train_acc: 0.493 test_loss: 0.8405, test_acc: 0.701\n",
      "Epoch: 2, Loss:0.7929, train_acc: 0.708 test_loss: 0.7953, test_acc: 0.708\n",
      "Epoch: 3, Loss:0.6374, train_acc: 0.764 test_loss: 0.5563, test_acc: 0.799\n",
      "Epoch: 4, Loss:0.5559, train_acc: 0.794 test_loss: 0.4974, test_acc: 0.818\n",
      "Epoch: 5, Loss:0.4981, train_acc: 0.817 test_loss: 0.4535, test_acc: 0.838\n",
      "Epoch: 6, Loss:0.4565, train_acc: 0.832 test_loss: 0.4111, test_acc: 0.855\n",
      "Epoch: 7, Loss:0.4250, train_acc: 0.843 test_loss: 0.3801, test_acc: 0.863\n",
      "Epoch: 8, Loss:0.4031, train_acc: 0.852 test_loss: 0.3818, test_acc: 0.860\n",
      "Epoch: 9, Loss:0.3854, train_acc: 0.859 test_loss: 0.3613, test_acc: 0.870\n",
      "Epoch: 10, Loss:0.3708, train_acc: 0.863 test_loss: 0.3471, test_acc: 0.877\n"
     ]
    }
   ],
   "source": [
    "# 训练要加分布式\n",
    "with strategy.scope():\n",
    "    # 数据是分布在各个GPU ,所以loss_func 不能在每个GPU求平均，要集合到一起求平均\n",
    "    #  设置： 1。 reduction=keras.losses.Reduction.NONE\n",
    "    #         2. compute_loss 方法求总的损失平均\n",
    "    loss_func = keras.losses.SparseCategoricalCrossentropy(reduction=keras.losses.Reduction.NONE)\n",
    "    \n",
    "    def compute_loss(labels, predictions):\n",
    "        loss_per_replica = loss_func(labels, predictions)\n",
    "        return tf.nn.compute_average_loss(loss_per_replica, global_batch_size=batch_size)\n",
    "    \n",
    "    test_loss = keras.metrics.Mean(name=\"test_loss\")\n",
    "    test_accuracy = keras.metrics.SparseCategoricalAccuracy(name=\"test_accuracy\")\n",
    "    train_accuracy = keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")\n",
    "\n",
    "    # 定义优化器\n",
    "    optimizer = keras.optimizers.SGD(lr=0.01)\n",
    "\n",
    "\n",
    "    def train_step(inputs):\n",
    "        images, labels = inputs\n",
    "\n",
    "        # 前向传播\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(images, training=True)\n",
    "            loss = compute_loss(labels, predictions)  # 记得改\n",
    "\n",
    "        # 反向传播\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        # 计算度量\n",
    "        train_accuracy.update_state(labels, predictions)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    # 分布式策略\n",
    "    @tf.function\n",
    "    def distributed_train_step(inputs):\n",
    "        average_loss_per_replica = strategy.experimental_run_v2(train_step, args=(inputs,))\n",
    "        return strategy.reduce(tf.distribute.ReduceOp.SUM, average_loss_per_replica, axis=None)\n",
    "\n",
    "\n",
    "    def test_step(inputs):\n",
    "        images, labels = inputs\n",
    "        predictions = model(images, training=False)\n",
    "        t_loss = loss_func(labels, predictions)\n",
    "        test_loss.update_state(t_loss)\n",
    "        test_accuracy.update_state(labels, predictions)\n",
    "    \n",
    "    # 加分布式策略\n",
    "    @tf.function\n",
    "    def distribated_test_step(inputs):\n",
    "        strategy.experimental_run_v2(test_step, args=(inputs, ))\n",
    "\n",
    "    # 遍历数据训练\n",
    "    epochs = 10\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        # 训练\n",
    "        for x in train_dataset_distribute: # 数据是分布式的\n",
    "            start_time = time.time()\n",
    "            total_loss += distributed_train_step(x)\n",
    "            run_time = time.time() - start_time\n",
    "            num_batches += 1\n",
    "            print(\"\\rtotal: {:.4f}, num: {}, average: {:.4f}, acc: {:.2f}, run_time: {:.4f}\".format(\n",
    "                total_loss, num_batches, total_loss/num_batches, train_accuracy.result(), run_time), end=\"\")\n",
    "\n",
    "        train_loss = total_loss/num_batches\n",
    "        \n",
    "        # 测试\n",
    "        for x in valid_dataset_distribute: # 数据是分布式的\n",
    "            distribated_test_step(x)\n",
    "\n",
    "        print(\"\\rEpoch: {}, Loss:{:.4f}, train_acc: {:.3f} test_loss: {:.4f}, test_acc: {:.3f}\".format(\n",
    "            epoch+1, train_loss, train_accuracy.result(), test_loss.result(),test_accuracy.result()))\n",
    "\n",
    "        # 清空度量，记得写\n",
    "        test_loss.reset_states()\n",
    "        train_accuracy.reset_states()\n",
    "        test_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
