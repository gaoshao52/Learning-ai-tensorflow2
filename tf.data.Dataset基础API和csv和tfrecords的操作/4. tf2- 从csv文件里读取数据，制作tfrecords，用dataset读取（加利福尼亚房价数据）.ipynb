{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目录\n",
    "## 1. 导入模块\n",
    "## 2. 从CSV文件里读取数据\n",
    "### 2.1 获取文件名列表\n",
    "### 2.2 从csv文件里读取数据，以dataset返回\n",
    "  - `tf.data.Dataset.list_files`\n",
    "  - `tf.data.Dataset.from_tensor_slices`\n",
    "  - `interleave`\n",
    "  - `tf.data.TextLineDataset` 和 `skip`\n",
    "  - `tf.io.decode_csv`\n",
    "  - `tf.stack`\n",
    "  - `map`\n",
    "  - `shuffle`\n",
    "  - `repeat`\n",
    "  - `batch`\n",
    "\n",
    "## 3. 把数据写入tf_records\n",
    "### 3.1 定义写入方法\n",
    "  - `tf.io.TFRecordOptions`\n",
    "  - `tf.io.TFRecordWriter`\n",
    "  - `skip`\n",
    "  - `tf.train.Example`\n",
    "  - `tf.train.Features`\n",
    "  - `tf.train.Feature`\n",
    "  - `tf.train.FloatList`\n",
    "  - `SerializeToString`\n",
    "  \n",
    "### 3.2 普通写入\n",
    "### 3.3 ZIP写入\n",
    "\n",
    "## 4. tfrecord 读取\n",
    "### 4.1 读取方法的定义\n",
    "  - `tf.data.Dataset.list_files`\n",
    "  - `interleave`\n",
    "  - `tf.data.TFRecordDataset`\n",
    "  - `tf.io.FixedLenFeature`\n",
    "  - `tf.io.parse_single_example`\n",
    "  - `map`\n",
    "  - `shuffle`\n",
    "  - `repeat`\n",
    "  - `batch`\n",
    "  \n",
    "### 4.2 测试\n",
    "### 4.3 train_dataset、valid_dataset、test_dataset的获取\n",
    "## 5. 模型建立、训练、测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy 1.18.1\n",
      "pandas 0.25.3\n",
      "matplotlib 3.1.2\n",
      "sklearn 0.22.1\n",
      "tensorflow_core.python.keras.api._v2.keras 2.2.4-tf\n",
      "tensorflow 2.1.0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "for module in [np, pd, mpl, sklearn, keras, tf]:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 从CSV文件里读取数据\n",
    ">第二个文件笔记生成的加利福尼亚房价csv数据，并按照该文件笔记的方法生成dataset读取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 获取文件名列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['generate_csv/train_11.csv', 'generate_csv/train_01.csv', 'generate_csv/train_19.csv', 'generate_csv/train_09.csv', 'generate_csv/train_05.csv', 'generate_csv/train_14.csv', 'generate_csv/train_15.csv', 'generate_csv/train_16.csv', 'generate_csv/train_10.csv', 'generate_csv/train_00.csv', 'generate_csv/train_17.csv', 'generate_csv/train_06.csv', 'generate_csv/train_03.csv', 'generate_csv/train_07.csv', 'generate_csv/train_12.csv', 'generate_csv/train_08.csv', 'generate_csv/train_18.csv', 'generate_csv/train_02.csv', 'generate_csv/train_04.csv', 'generate_csv/train_13.csv']\n",
      "['generate_csv/valid_04.csv', 'generate_csv/valid_08.csv', 'generate_csv/valid_07.csv', 'generate_csv/valid_06.csv', 'generate_csv/valid_01.csv', 'generate_csv/valid_02.csv', 'generate_csv/valid_03.csv', 'generate_csv/valid_00.csv', 'generate_csv/valid_09.csv', 'generate_csv/valid_05.csv']\n",
      "['generate_csv/test_05.csv', 'generate_csv/test_06.csv', 'generate_csv/test_08.csv', 'generate_csv/test_02.csv', 'generate_csv/test_00.csv', 'generate_csv/test_04.csv', 'generate_csv/test_09.csv', 'generate_csv/test_07.csv', 'generate_csv/test_01.csv', 'generate_csv/test_03.csv']\n"
     ]
    }
   ],
   "source": [
    "def get_filenames_by_prefix(source_dir, prefix_name):\n",
    "    all_file = os.listdir(source_dir)\n",
    "    return [os.path.join(source_dir, file) for file in all_file if file.startswith(prefix_name) and file.endswith(\".csv\")]\n",
    "\n",
    "train_filenames = get_filenames_by_prefix(\"generate_csv/\", \"train\")\n",
    "valid_filenames = get_filenames_by_prefix(\"generate_csv/\", \"valid\")\n",
    "test_filenames = get_filenames_by_prefix(\"generate_csv/\", \"test\")\n",
    "\n",
    "print(train_filenames)\n",
    "print(valid_filenames)\n",
    "print(test_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 从csv文件里读取数据，以dataset返回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_reader_dataset(filenames, n_readers=5, n_fields=9, n_threads=5, epochs=None, batch_size=32, shuffle=True):\n",
    "\n",
    "    # 1. dataset 分装文件列表\n",
    "    filenames_dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "    # filenames_dataset = tf.data.Dataset.list_files(filenames)\n",
    "    \n",
    "    # 2. interleave 根据文件名 读取文件数据\n",
    "    raw_dataset = filenames_dataset.interleave( # interleave 下面函数返回的元素，组合成一个dataset\n",
    "        lambda filename: tf.data.TextLineDataset(filename).skip(1),\n",
    "        cycle_length=n_readers\n",
    "    )\n",
    "    \n",
    "    # 3. 使用map 和 tf.io.decode_csv 对应解析\n",
    "    def parse_csv_line(line, n_fields):\n",
    "        default_fields = [tf.constant(np.nan)]* n_fields\n",
    "        parsed_line = tf.io.decode_csv(line, default_fields) # 解析出来的是一个列表，列表的元素是一个0维的tensor\n",
    "\n",
    "        # tf.stack 把 parsed_line 转化为一个向量tensor\n",
    "        x = tf.stack(parsed_line[:-1])\n",
    "        y = tf.stack(parsed_line[-1:])\n",
    "\n",
    "        return x, y\n",
    "    \n",
    "    dataset = raw_dataset.map(  # map 不改变dataset的元素数量，只改变元素的数据结构\n",
    "        map_func=lambda line: parse_csv_line(line, n_fields),\n",
    "        num_parallel_calls=n_threads\n",
    "    )\n",
    "    \n",
    "    # 4. shuffle repeat batch\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(10000)\n",
    "    dataset = dataset.repeat(epochs).batch(batch_size)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "train_dataset = csv_reader_dataset(train_filenames, n_readers=5, n_fields=9, n_threads=5, epochs=1, batch_size=32, shuffle=False)\n",
    "valid_dataset = csv_reader_dataset(valid_filenames, n_readers=5, n_fields=9, n_threads=5, epochs=1, batch_size=32, shuffle=False)\n",
    "test_dataset = csv_reader_dataset(test_filenames, n_readers=5, n_fields=9, n_threads=5, epochs=1, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 把数据写入tf_records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 定义写入方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_dataset_to_tfrecords(base_filenames, dataset, n_shares, steps_per_share, compression_type=None):\n",
    "    \n",
    "    all_filenames = []\n",
    "    for share_id in range(n_shares):\n",
    "        \n",
    "        # 写入文件的路径\n",
    "        filename_fullpath = \"{}_{:05d}-of-{:05d}\".format(base_filenames, share_id, n_shares)\n",
    "        all_filenames.append(filename_fullpath)\n",
    "        \n",
    "        # 获取tf_record 写入对象\n",
    "        options = tf.io.TFRecordOptions(compression_type=compression_type)\n",
    "        with tf.io.TFRecordWriter(filename_fullpath, options=options) as writer:\n",
    "            \n",
    "            # 从dataset里遍历数据\n",
    "            for batch_x, batch_y in dataset.skip(share_id*steps_per_share).take(steps_per_share):\n",
    "                \n",
    "                # 对一个batch的数据再次遍历\n",
    "                for example_x, example_y in zip(batch_x, batch_y):\n",
    "                    \n",
    "                    # 对一个样本数据分装成example\n",
    "                    example = tf.train.Example(\n",
    "                        features=tf.train.Features(\n",
    "                            feature={\n",
    "                                \"input_features\": tf.train.Feature(float_list=tf.train.FloatList(value=example_x)),\n",
    "                                \"label\": tf.train.Feature(float_list=tf.train.FloatList(value=example_y))\n",
    "                            }\n",
    "                        )\n",
    "                    )\n",
    "                    \n",
    "                    # 序列化example\n",
    "                    serialized_example = example.SerializeToString()\n",
    "                    \n",
    "                    # 执行写操作\n",
    "                    writer.write(serialized_example)\n",
    "    return all_filenames\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 普通写入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['generate_tfrecords/train_00000-of-00020', 'generate_tfrecords/train_00001-of-00020', 'generate_tfrecords/train_00002-of-00020', 'generate_tfrecords/train_00003-of-00020', 'generate_tfrecords/train_00004-of-00020', 'generate_tfrecords/train_00005-of-00020', 'generate_tfrecords/train_00006-of-00020', 'generate_tfrecords/train_00007-of-00020', 'generate_tfrecords/train_00008-of-00020', 'generate_tfrecords/train_00009-of-00020', 'generate_tfrecords/train_00010-of-00020', 'generate_tfrecords/train_00011-of-00020', 'generate_tfrecords/train_00012-of-00020', 'generate_tfrecords/train_00013-of-00020', 'generate_tfrecords/train_00014-of-00020', 'generate_tfrecords/train_00015-of-00020', 'generate_tfrecords/train_00016-of-00020', 'generate_tfrecords/train_00017-of-00020', 'generate_tfrecords/train_00018-of-00020', 'generate_tfrecords/train_00019-of-00020']\n",
      "['generate_tfrecords/valid_00000-of-00020', 'generate_tfrecords/valid_00001-of-00020', 'generate_tfrecords/valid_00002-of-00020', 'generate_tfrecords/valid_00003-of-00020', 'generate_tfrecords/valid_00004-of-00020', 'generate_tfrecords/valid_00005-of-00020', 'generate_tfrecords/valid_00006-of-00020', 'generate_tfrecords/valid_00007-of-00020', 'generate_tfrecords/valid_00008-of-00020', 'generate_tfrecords/valid_00009-of-00020', 'generate_tfrecords/valid_00010-of-00020', 'generate_tfrecords/valid_00011-of-00020', 'generate_tfrecords/valid_00012-of-00020', 'generate_tfrecords/valid_00013-of-00020', 'generate_tfrecords/valid_00014-of-00020', 'generate_tfrecords/valid_00015-of-00020', 'generate_tfrecords/valid_00016-of-00020', 'generate_tfrecords/valid_00017-of-00020', 'generate_tfrecords/valid_00018-of-00020', 'generate_tfrecords/valid_00019-of-00020']\n",
      "['generate_tfrecords/test_00000-of-00020', 'generate_tfrecords/test_00001-of-00020', 'generate_tfrecords/test_00002-of-00020', 'generate_tfrecords/test_00003-of-00020', 'generate_tfrecords/test_00004-of-00020', 'generate_tfrecords/test_00005-of-00020', 'generate_tfrecords/test_00006-of-00020', 'generate_tfrecords/test_00007-of-00020', 'generate_tfrecords/test_00008-of-00020', 'generate_tfrecords/test_00009-of-00020', 'generate_tfrecords/test_00010-of-00020', 'generate_tfrecords/test_00011-of-00020', 'generate_tfrecords/test_00012-of-00020', 'generate_tfrecords/test_00013-of-00020', 'generate_tfrecords/test_00014-of-00020', 'generate_tfrecords/test_00015-of-00020', 'generate_tfrecords/test_00016-of-00020', 'generate_tfrecords/test_00017-of-00020', 'generate_tfrecords/test_00018-of-00020', 'generate_tfrecords/test_00019-of-00020']\n"
     ]
    }
   ],
   "source": [
    "n_shares = 20\n",
    "batch_size = 32\n",
    "train_steps_per_share = 11610 //batch_size//n_shares\n",
    "valid_steps_per_share = 3880 //batch_size//n_shares\n",
    "test_steps_per_share = 5170 //batch_size//n_shares\n",
    "\n",
    "\n",
    "output_dir = \"generate_tfrecords\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "    \n",
    "train_basename = os.path.join(output_dir, \"train\")\n",
    "valid_basename = os.path.join(output_dir, \"valid\")\n",
    "test_basename = os.path.join(output_dir, \"test\")\n",
    "\n",
    "train_tfrecords_filenames = csv_dataset_to_tfrecords(train_basename, train_dataset, n_shares, train_steps_per_share, compression_type=None)\n",
    "valid_tfrecords_filenames = csv_dataset_to_tfrecords(valid_basename, valid_dataset, n_shares, valid_steps_per_share, compression_type=None)\n",
    "test_tfrecords_filenames = csv_dataset_to_tfrecords(test_basename, test_dataset, n_shares, test_steps_per_share, compression_type=None)\n",
    "\n",
    "print(train_tfrecords_filenames)\n",
    "print(valid_tfrecords_filenames)\n",
    "print(test_tfrecords_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 ZIP写入 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['generate_tfrecords_zip/train_00000-of-00020', 'generate_tfrecords_zip/train_00001-of-00020', 'generate_tfrecords_zip/train_00002-of-00020', 'generate_tfrecords_zip/train_00003-of-00020', 'generate_tfrecords_zip/train_00004-of-00020', 'generate_tfrecords_zip/train_00005-of-00020', 'generate_tfrecords_zip/train_00006-of-00020', 'generate_tfrecords_zip/train_00007-of-00020', 'generate_tfrecords_zip/train_00008-of-00020', 'generate_tfrecords_zip/train_00009-of-00020', 'generate_tfrecords_zip/train_00010-of-00020', 'generate_tfrecords_zip/train_00011-of-00020', 'generate_tfrecords_zip/train_00012-of-00020', 'generate_tfrecords_zip/train_00013-of-00020', 'generate_tfrecords_zip/train_00014-of-00020', 'generate_tfrecords_zip/train_00015-of-00020', 'generate_tfrecords_zip/train_00016-of-00020', 'generate_tfrecords_zip/train_00017-of-00020', 'generate_tfrecords_zip/train_00018-of-00020', 'generate_tfrecords_zip/train_00019-of-00020']\n",
      "['generate_tfrecords_zip/valid_00000-of-00020', 'generate_tfrecords_zip/valid_00001-of-00020', 'generate_tfrecords_zip/valid_00002-of-00020', 'generate_tfrecords_zip/valid_00003-of-00020', 'generate_tfrecords_zip/valid_00004-of-00020', 'generate_tfrecords_zip/valid_00005-of-00020', 'generate_tfrecords_zip/valid_00006-of-00020', 'generate_tfrecords_zip/valid_00007-of-00020', 'generate_tfrecords_zip/valid_00008-of-00020', 'generate_tfrecords_zip/valid_00009-of-00020', 'generate_tfrecords_zip/valid_00010-of-00020', 'generate_tfrecords_zip/valid_00011-of-00020', 'generate_tfrecords_zip/valid_00012-of-00020', 'generate_tfrecords_zip/valid_00013-of-00020', 'generate_tfrecords_zip/valid_00014-of-00020', 'generate_tfrecords_zip/valid_00015-of-00020', 'generate_tfrecords_zip/valid_00016-of-00020', 'generate_tfrecords_zip/valid_00017-of-00020', 'generate_tfrecords_zip/valid_00018-of-00020', 'generate_tfrecords_zip/valid_00019-of-00020']\n",
      "['generate_tfrecords_zip/test_00000-of-00020', 'generate_tfrecords_zip/test_00001-of-00020', 'generate_tfrecords_zip/test_00002-of-00020', 'generate_tfrecords_zip/test_00003-of-00020', 'generate_tfrecords_zip/test_00004-of-00020', 'generate_tfrecords_zip/test_00005-of-00020', 'generate_tfrecords_zip/test_00006-of-00020', 'generate_tfrecords_zip/test_00007-of-00020', 'generate_tfrecords_zip/test_00008-of-00020', 'generate_tfrecords_zip/test_00009-of-00020', 'generate_tfrecords_zip/test_00010-of-00020', 'generate_tfrecords_zip/test_00011-of-00020', 'generate_tfrecords_zip/test_00012-of-00020', 'generate_tfrecords_zip/test_00013-of-00020', 'generate_tfrecords_zip/test_00014-of-00020', 'generate_tfrecords_zip/test_00015-of-00020', 'generate_tfrecords_zip/test_00016-of-00020', 'generate_tfrecords_zip/test_00017-of-00020', 'generate_tfrecords_zip/test_00018-of-00020', 'generate_tfrecords_zip/test_00019-of-00020']\n"
     ]
    }
   ],
   "source": [
    "zip_output_dir = \"generate_tfrecords_zip\"\n",
    "if not os.path.exists(zip_output_dir):\n",
    "    os.mkdir(zip_output_dir)\n",
    "    \n",
    "zip_train_basename = os.path.join(zip_output_dir, \"train\")\n",
    "zip_valid_basename = os.path.join(zip_output_dir, \"valid\")\n",
    "zip_test_basename = os.path.join(zip_output_dir, \"test\")\n",
    "\n",
    "train_tfrecords_filenames_zip = csv_dataset_to_tfrecords(\n",
    "    zip_train_basename, train_dataset, n_shares, train_steps_per_share, compression_type=\"GZIP\")\n",
    "valid_tfrecords_filenames_zip = csv_dataset_to_tfrecords(\n",
    "    zip_valid_basename, valid_dataset, n_shares, valid_steps_per_share, compression_type=\"GZIP\")\n",
    "test_tfrecords_filenames_zip = csv_dataset_to_tfrecords(\n",
    "    zip_test_basename, test_dataset, n_shares, test_steps_per_share, compression_type=\"GZIP\")\n",
    "\n",
    "print(train_tfrecords_filenames_zip)\n",
    "print(valid_tfrecords_filenames_zip)\n",
    "print(test_tfrecords_filenames_zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. tfrecord 读取\n",
    ">套路跟`csv_reader_dataset`方法一样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 读取方法的定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfrecords_reader_dataset(\n",
    "    filenames, n_readers=5,n_threads=5, epochs=None, batch_size=32, shuffle=True, compression_type=None):\n",
    "\n",
    "    # 1. dataset 分装文件列表\n",
    "    #filenames_dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "    filenames_dataset = tf.data.Dataset.list_files(filenames)\n",
    "    \n",
    "    # 2. interleave 根据文件名 读取文件数据\n",
    "    raw_dataset = filenames_dataset.interleave( # interleave 下面函数返回的元素，组合成一个dataset\n",
    "        lambda filename: tf.data.TFRecordDataset(filename, compression_type=compression_type),\n",
    "        cycle_length=n_readers\n",
    "    )\n",
    "    \n",
    "    # 3. 使用map 和 tf.io.parse_single_example 对应解析\n",
    "    def parse_serialized_example(serialized_example):\n",
    "        expected_features = {\n",
    "            \"input_features\": tf.io.FixedLenFeature([8], dtype=tf.float32),\n",
    "            \"label\": tf.io.FixedLenFeature([1], dtype=tf.float32)\n",
    "        }\n",
    "        example = tf.io.parse_single_example(serialized_example, expected_features)\n",
    "        return example[\"input_features\"], example[\"label\"]\n",
    "        \n",
    "    \n",
    "    dataset = raw_dataset.map(  # map 不改变dataset的元素数量，只改变元素的数据结构\n",
    "        map_func=parse_serialized_example,\n",
    "        num_parallel_calls=n_threads\n",
    "    )\n",
    "    \n",
    "    # 4. shuffle repeat batch\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(10000)\n",
    "    dataset = dataset.repeat(epochs).batch(batch_size)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.17960499  1.8773133  -0.19291258 -0.01163158 -0.7089215  -0.09591901\n",
      "   1.0027343  -1.347747  ]\n",
      " [-0.39126503 -0.12413459 -0.4806664  -0.09861694  0.6785487   0.01911838\n",
      "  -0.8130038   0.6002651 ]], shape=(2, 8), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[2.75]\n",
      " [2.15]], shape=(2, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 1.2281055   1.7171974   0.17694804  0.06742893 -0.6835753  -0.03524943\n",
      "  -0.7334482   0.5902753 ]\n",
      " [ 0.47200948  0.4362708   0.24721687 -0.18787888 -0.10248999  0.00257246\n",
      "  -0.8036443   0.7700918 ]], shape=(2, 8), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[5.00001]\n",
      " [2.107  ]], shape=(2, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 0.6331164   0.3562129   0.15536398 -0.10958562 -0.63851535  0.00521998\n",
      "  -0.6398535   0.5503161 ]\n",
      " [-0.5909521   1.6371396  -0.81227165 -0.10447624 -0.00673764  0.13667765\n",
      "  -0.75216717  0.68018353]], shape=(2, 8), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[2.103]\n",
      " [1.46 ]], shape=(2, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test_dataset= tfrecords_reader_dataset(\n",
    "    test_tfrecords_filenames_zip, n_readers=5,n_threads=5, epochs=1, batch_size=2, shuffle=False, compression_type=\"GZIP\")\n",
    "\n",
    "for x, y in test_dataset.take(3):\n",
    "    print(x)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 train_dataset、valid_dataset、test_dataset的获取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset= tfrecords_reader_dataset(\n",
    "    train_tfrecords_filenames_zip, n_readers=5,n_threads=5, epochs=None, batch_size=32, shuffle=True, compression_type=\"GZIP\")\n",
    "\n",
    "valid_dataset= tfrecords_reader_dataset(\n",
    "    valid_tfrecords_filenames_zip, n_readers=5,n_threads=5, epochs=1, batch_size=32, shuffle=False, compression_type=\"GZIP\")\n",
    "\n",
    "test_dataset= tfrecords_reader_dataset(\n",
    "    test_tfrecords_filenames_zip, n_readers=5,n_threads=5, epochs=1, batch_size=32, shuffle=False, compression_type=\"GZIP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 模型建立、训练、测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(100, input_shape=[8], activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 348 steps, validate for 120 steps\n",
      "Epoch 1/100\n",
      "348/348 [==============================] - 1s 3ms/step - loss: 0.7141 - val_loss: 0.7325\n",
      "Epoch 2/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.8888 - val_loss: 0.4738\n",
      "Epoch 3/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.4548 - val_loss: 1.0081\n",
      "Epoch 4/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.5834 - val_loss: 0.5594\n",
      "Epoch 5/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.4102 - val_loss: 0.4088\n",
      "Epoch 6/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 1.1638 - val_loss: 0.4572\n",
      "Epoch 7/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.3980 - val_loss: 0.3726\n",
      "Epoch 8/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.3460 - val_loss: 0.3450\n",
      "Epoch 9/100\n",
      "348/348 [==============================] - 1s 3ms/step - loss: 0.3431 - val_loss: 0.3431\n",
      "Epoch 10/100\n",
      "348/348 [==============================] - 1s 3ms/step - loss: 0.3294 - val_loss: 0.3240\n",
      "Epoch 11/100\n",
      "348/348 [==============================] - 1s 3ms/step - loss: 0.3242 - val_loss: 0.3237\n",
      "Epoch 12/100\n",
      "348/348 [==============================] - 1s 3ms/step - loss: 0.3157 - val_loss: 0.3136\n",
      "Epoch 13/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.3169 - val_loss: 0.3135\n",
      "Epoch 14/100\n",
      "348/348 [==============================] - 1s 3ms/step - loss: 0.3106 - val_loss: 0.3048\n",
      "Epoch 15/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2956 - val_loss: 0.3060\n",
      "Epoch 16/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.3023 - val_loss: 0.3069\n",
      "Epoch 17/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2990 - val_loss: 0.3416\n",
      "Epoch 18/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2908 - val_loss: 0.2954\n",
      "Epoch 19/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2962 - val_loss: 0.3057\n",
      "Epoch 20/100\n",
      "348/348 [==============================] - 1s 3ms/step - loss: 0.3018 - val_loss: 0.2940\n",
      "Epoch 21/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2890 - val_loss: 0.2927\n",
      "Epoch 22/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2854 - val_loss: 0.2939\n",
      "Epoch 23/100\n",
      "348/348 [==============================] - 1s 3ms/step - loss: 0.2915 - val_loss: 0.2936\n",
      "Epoch 24/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2781 - val_loss: 0.3022\n",
      "Epoch 25/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2830 - val_loss: 0.2954\n",
      "Epoch 26/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2836 - val_loss: 0.2896\n",
      "Epoch 27/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2739 - val_loss: 0.2862\n",
      "Epoch 28/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2768 - val_loss: 0.2981\n",
      "Epoch 29/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2797 - val_loss: 0.2883\n",
      "Epoch 30/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2743 - val_loss: 0.2845\n",
      "Epoch 31/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2748 - val_loss: 0.2904\n",
      "Epoch 32/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2708 - val_loss: 0.3215\n",
      "Epoch 33/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2721 - val_loss: 0.2801\n",
      "Epoch 34/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2769 - val_loss: 0.2973\n",
      "Epoch 35/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2674 - val_loss: 0.2841\n",
      "Epoch 36/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2751 - val_loss: 0.2832\n",
      "Epoch 37/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2729 - val_loss: 0.2802\n",
      "Epoch 38/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2631 - val_loss: 0.2924\n",
      "Epoch 39/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2637 - val_loss: 0.2779\n",
      "Epoch 40/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2723 - val_loss: 0.2814\n",
      "Epoch 41/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2681 - val_loss: 0.2781\n",
      "Epoch 42/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2647 - val_loss: 0.2856\n",
      "Epoch 43/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2622 - val_loss: 0.2780\n",
      "Epoch 44/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2671 - val_loss: 0.2780\n",
      "Epoch 45/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2626 - val_loss: 0.2854\n",
      "Epoch 46/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2619 - val_loss: 0.2771\n",
      "Epoch 47/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2544 - val_loss: 0.2743\n",
      "Epoch 48/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2724 - val_loss: 0.2862\n",
      "Epoch 49/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2625 - val_loss: 0.2869\n",
      "Epoch 50/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2585 - val_loss: 0.2819\n",
      "Epoch 51/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2596 - val_loss: 0.2830\n",
      "Epoch 52/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2639 - val_loss: 0.2754\n",
      "Epoch 53/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2574 - val_loss: 0.2789\n",
      "Epoch 54/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2642 - val_loss: 0.2729\n",
      "Epoch 55/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2508 - val_loss: 0.2705\n",
      "Epoch 56/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2596 - val_loss: 0.2785\n",
      "Epoch 57/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2559 - val_loss: 0.2754\n",
      "Epoch 58/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2544 - val_loss: 0.2728\n",
      "Epoch 59/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2573 - val_loss: 0.2769\n",
      "Epoch 60/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2541 - val_loss: 0.2764\n",
      "Epoch 61/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2537 - val_loss: 0.2811\n",
      "Epoch 62/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2590 - val_loss: 0.2718\n",
      "Epoch 63/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2467 - val_loss: 0.2733\n",
      "Epoch 64/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2551 - val_loss: 0.2710\n",
      "Epoch 65/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2522 - val_loss: 0.2719\n",
      "Epoch 66/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2555 - val_loss: 0.2916\n",
      "Epoch 67/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2480 - val_loss: 0.2855\n",
      "Epoch 68/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2528 - val_loss: 0.2754\n",
      "Epoch 69/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2498 - val_loss: 0.2726\n",
      "Epoch 70/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2474 - val_loss: 0.2756\n",
      "Epoch 71/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2469 - val_loss: 0.2761\n",
      "Epoch 72/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2575 - val_loss: 0.2720\n",
      "Epoch 73/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2393 - val_loss: 0.2776\n",
      "Epoch 74/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2521 - val_loss: 0.2762\n",
      "Epoch 75/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2521 - val_loss: 0.2751\n",
      "Epoch 76/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2521 - val_loss: 0.2766\n",
      "Epoch 77/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2452 - val_loss: 0.2731\n",
      "Epoch 78/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2421 - val_loss: 0.2840\n",
      "Epoch 79/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2484 - val_loss: 0.2727\n",
      "Epoch 80/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2512 - val_loss: 0.2744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2395 - val_loss: 0.2729\n",
      "Epoch 82/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2451 - val_loss: 0.2703\n",
      "Epoch 83/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2497 - val_loss: 0.2831\n",
      "Epoch 84/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2465 - val_loss: 0.2659\n",
      "Epoch 85/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2369 - val_loss: 0.2804\n",
      "Epoch 86/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2473 - val_loss: 0.2733\n",
      "Epoch 87/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2454 - val_loss: 0.2712\n",
      "Epoch 88/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2449 - val_loss: 0.2719\n",
      "Epoch 89/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2421 - val_loss: 0.2744\n",
      "Epoch 90/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2410 - val_loss: 0.2719\n",
      "Epoch 91/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2425 - val_loss: 0.2732\n",
      "Epoch 92/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2413 - val_loss: 0.2668\n",
      "Epoch 93/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2435 - val_loss: 0.2679\n",
      "Epoch 94/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2411 - val_loss: 0.2700\n",
      "Epoch 95/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2421 - val_loss: 0.2650\n",
      "Epoch 96/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2396 - val_loss: 0.2753\n",
      "Epoch 97/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2405 - val_loss: 0.2659\n",
      "Epoch 98/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2395 - val_loss: 0.2665\n",
      "Epoch 99/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2405 - val_loss: 0.2671\n",
      "Epoch 100/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2408 - val_loss: 0.2679\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe9a00f5f60>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_dataset, \n",
    "    validation_data=valid_dataset, \n",
    "    steps_per_epoch=11160//32, \n",
    "    validation_steps=3870//32, \n",
    "    epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    160/Unknown - 0s 2ms/step - loss: 0.2834"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2834040036890656"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
