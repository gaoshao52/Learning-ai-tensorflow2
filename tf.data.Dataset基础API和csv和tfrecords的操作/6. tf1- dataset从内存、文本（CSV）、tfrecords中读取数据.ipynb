{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目录\n",
    "## 1. 模块导入\n",
    "## 2. dataset从内存中读取数据\n",
    "  - `tf.data.Dataset.from_tensor_slices`\n",
    "  - `make_one_shot_iterator`\n",
    "  - `get_next`\n",
    "  \n",
    "## 3. dataset从文件（CSV）读取数据\n",
    "  - ### 3.1 生成CSV文件\n",
    "  - ### 3.2 读取CSV文件\n",
    "    - `tf.data.Dataset.list_files`\n",
    "\t- `interleave` 和 `tf.data.TextLineDataset` 和 `skip`\n",
    "\t- `map` 和 `tf.io.decode_csv`\n",
    "\t- `make_one_shot_iterator`\n",
    "\t- `get_next`\n",
    "\n",
    "## 4. dataset从tf_records读取数据\n",
    "  - ### 4.1 制作mnist数据tf_records\n",
    "    - `np.array_split`\n",
    "\t- `tf.python_io.TFRecordWriter`\n",
    "\t- `tf.train.Example` 和 `tf.train.Features` 和 `tf.train.Feature` 和 `tf.train.BytesList` 和 `tf.train.Int64List`\n",
    "\t- `SerializeToString`\n",
    "  - ### 4.2 读取tf_records数据\n",
    "    - `tf.data.Dataset.list_files`\n",
    "\t- `interleave` 和 `tf.data.TFRecordDataset` \n",
    "\t- `map` 和 `tf.io.parse_single_example` `tf.io.FixedLenFeature` `tf.io.decode_raw`\n",
    "\t- `shuffle`\n",
    "\t- `repeat`\n",
    "\t- `batch`\n",
    "\t- `make_one_shot_iterator`\n",
    "\t- `get_next`\n",
    "    \n",
    "## 5. initializable_iterator动态初始化数据\n",
    "  - `placeholder`\n",
    "  - `tf.data.Dataset.list_files`\n",
    "  - `interleave` 和 `tf.data.TextLineDataset` 和 `skip`\n",
    "  - `map` 和 `tf.io.decode_csv`\n",
    "  - `make_initializable_iterator`\n",
    "  - `get_next`\n",
    "  - `initializer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy 1.17.2\n",
      "pandas 0.25.1\n",
      "matplotlib 3.1.1\n",
      "sklearn 0.21.3\n",
      "tensorflow.python.keras.api._v1.keras 2.2.4-tf\n",
      "tensorflow 1.15.0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "for module in [np, pd, mpl, sklearn, keras, tf]:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. dataset从内存中读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-c823538e9401>:4: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "1.0\n",
      "2.4\n",
      "6.9\n"
     ]
    }
   ],
   "source": [
    "array_data = np.array([1.0, 2.4, 6.9])\n",
    "dataset = tf.data.Dataset.from_tensor_slices(array_data)\n",
    "\n",
    "dataset_iter = dataset.make_one_shot_iterator()\n",
    "item = dataset_iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for _ in range(3):\n",
    "        value = sess.run(item)\n",
    "        print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. dataset从文件（CSV）读取数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - ### 3.1 生成CSV文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"tf1_dataset\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "csv_path01 = os.path.join(output_dir, \"data_01.csv\")\n",
    "csv_path02 = os.path.join(output_dir, \"data_02.csv\")\n",
    "\n",
    "with open(csv_path01, \"wt\") as f:\n",
    "    f.write(\"age,sex,salary\\n\")\n",
    "    f.write(\"23,men,2334.8\\n\")\n",
    "    f.write(\"24,men,5433.7\\n\")\n",
    "    f.write(\"25,women,7334.8\")\n",
    "    \n",
    "with open(csv_path02, \"wt\") as f:\n",
    "    f.write(\"age,sex,salary\\n\")\n",
    "    f.write(\"26,men,2334.8\\n\")\n",
    "    f.write(\"27,men,5433.7\\n\")\n",
    "    f.write(\"28,women,7334.8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>men</td>\n",
       "      <td>2334.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>men</td>\n",
       "      <td>5433.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>women</td>\n",
       "      <td>7334.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age    sex  salary\n",
       "0   23    men  2334.8\n",
       "1   24    men  5433.7\n",
       "2   25  women  7334.8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data1 = pd.read_csv(csv_path01)\n",
    "df_data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>men</td>\n",
       "      <td>2334.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>men</td>\n",
       "      <td>5433.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>women</td>\n",
       "      <td>7334.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age    sex  salary\n",
       "0   26    men  2334.8\n",
       "1   27    men  5433.7\n",
       "2   28  women  7334.8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data2 = pd.read_csv(csv_path02)\n",
    "df_data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - ### 3.2 读取CSV文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: age, type: <class 'numpy.int64'>, value: 23\n",
      "name: sex, type: <class 'bytes'>, value: b'men'\n",
      "name: salary, type: <class 'numpy.float32'>, value: 2334.800048828125\n",
      "name: age, type: <class 'numpy.int64'>, value: 26\n",
      "name: sex, type: <class 'bytes'>, value: b'men'\n",
      "name: salary, type: <class 'numpy.float32'>, value: 2334.800048828125\n",
      "name: age, type: <class 'numpy.int64'>, value: 24\n",
      "name: sex, type: <class 'bytes'>, value: b'men'\n",
      "name: salary, type: <class 'numpy.float32'>, value: 5433.7001953125\n",
      "name: age, type: <class 'numpy.int64'>, value: 27\n",
      "name: sex, type: <class 'bytes'>, value: b'men'\n",
      "name: salary, type: <class 'numpy.float32'>, value: 5433.7001953125\n",
      "name: age, type: <class 'numpy.int64'>, value: 25\n",
      "name: sex, type: <class 'bytes'>, value: b'women'\n",
      "name: salary, type: <class 'numpy.float32'>, value: 7334.7998046875\n",
      "name: age, type: <class 'numpy.int64'>, value: 28\n",
      "name: sex, type: <class 'bytes'>, value: b'women'\n",
      "name: salary, type: <class 'numpy.float32'>, value: 7334.7998046875\n"
     ]
    }
   ],
   "source": [
    "# csv_dataset = tf.data.TextLineDataset([csv_path01, csv_path02]).skip(1)  这样有问题，因为第一个文件header能跳过，第二个文件名就跳不过了\n",
    "\n",
    "# 文件列表dataset\n",
    "filename_dataset = tf.data.Dataset.list_files([csv_path01, csv_path02])\n",
    "\n",
    "# 根据文件名，读取数据，聚合成一个新的dataset\n",
    "csv_dataset = filename_dataset.interleave(\n",
    "    lambda filename: tf.data.TextLineDataset(filename).skip(1)\n",
    ")\n",
    "\n",
    "# 解析\n",
    "def parse_csv(line):\n",
    "    default_fields = [tf.constant(0, dtype=tf.int64), tf.constant(\"gg\", dtype=tf.string), tf.constant(0.0, dtype=tf.float32)]\n",
    "    parse_line = tf.io.decode_csv(line, default_fields) # 返回的都是list结构，元素都是tensor\n",
    "    age = parse_line[0]\n",
    "    sex = parse_line[1]\n",
    "    salary = parse_line[2]\n",
    "    return age, sex, salary\n",
    "\n",
    "# 对dataset元素处理，不改变元素数量\n",
    "dataset = csv_dataset.map(parse_csv)\n",
    "\n",
    "# 定义迭代器\n",
    "dataset_iter = dataset.make_one_shot_iterator()\n",
    "\n",
    "# 获取一个迭代元素\n",
    "age, sex, salary = dataset_iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for _ in range(6):\n",
    "        age_value, sex_value, salary_value = sess.run([age, sex, salary])\n",
    "        print(\"name: age, type: {}, value: {}\".format(type(age_value), age_value))\n",
    "        print(\"name: sex, type: {}, value: {}\".format(type(sex_value), sex_value))\n",
    "        print(\"name: salary, type: {}, value: {}\".format(type(salary_value), salary_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. dataset从tf_records读取数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - ### 4.1 制作mnist数据tf_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-1f4682a7cf61>:7: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/gaosy/environment/tf1.14_py3/lib/python3.6/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/gaosy/environment/tf1.14_py3/lib/python3.6/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting tf1_data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/gaosy/environment/tf1.14_py3/lib/python3.6/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting tf1_data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting tf1_data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting tf1_data/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/gaosy/environment/tf1.14_py3/lib/python3.6/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "output_dir = \"tf1_data\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "mnist = input_data.read_data_sets(\"tf1_data/MNIST_data\", dtype=tf.uint8, one_hot=False)\n",
    "\n",
    "def mnist_to_tfrecords(images, labels, save_dir, name_prefix, n_parts):\n",
    "    path_format = os.path.join(save_dir, \"{}_{:02d}-of-{:02d}.tfrecords\")\n",
    "    all_filenames = []\n",
    "    \n",
    "    for file_index, row_indices in enumerate(np.array_split(np.arange(len(images)), n_parts)):\n",
    "        filename_fullpath = path_format.format(name_prefix, file_index, n_parts)\n",
    "        all_filenames.append(filename_fullpath)\n",
    "        \n",
    "        with tf.python_io.TFRecordWriter(filename_fullpath) as writer:\n",
    "            for row_index in row_indices:\n",
    "                example = tf.train.Example(\n",
    "                    features=tf.train.Features(\n",
    "                        feature={\n",
    "                            \"image\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[images[row_index].tostring()])),\n",
    "                            \"label\": tf.train.Feature(int64_list=tf.train.Int64List(value=[labels[row_index]]))\n",
    "                        }\n",
    "                    )\n",
    "                )\n",
    "                serialized_example = example.SerializeToString()\n",
    "                writer.write(serialized_example)\n",
    "    return all_filenames      \n",
    "\n",
    "train_filenames = mnist_to_tfrecords(mnist.train.images, mnist.train.labels, output_dir, \"train\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tf1_data/train_00-of-10.tfrecords',\n",
       " 'tf1_data/train_01-of-10.tfrecords',\n",
       " 'tf1_data/train_02-of-10.tfrecords',\n",
       " 'tf1_data/train_03-of-10.tfrecords',\n",
       " 'tf1_data/train_04-of-10.tfrecords',\n",
       " 'tf1_data/train_05-of-10.tfrecords',\n",
       " 'tf1_data/train_06-of-10.tfrecords',\n",
       " 'tf1_data/train_07-of-10.tfrecords',\n",
       " 'tf1_data/train_08-of-10.tfrecords',\n",
       " 'tf1_data/train_09-of-10.tfrecords']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - ### 4.2 读取tf_records数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images: (32, 784), labels: (32,)\n",
      "images: (32, 784), labels: (32,)\n",
      "images: (32, 784), labels: (32,)\n",
      "images: (32, 784), labels: (32,)\n",
      "images: (32, 784), labels: (32,)\n",
      "images: (32, 784), labels: (32,)\n",
      "images: (32, 784), labels: (32,)\n",
      "images: (32, 784), labels: (32,)\n",
      "images: (32, 784), labels: (32,)\n",
      "images: (32, 784), labels: (32,)\n"
     ]
    }
   ],
   "source": [
    "# 制作文件dataset\n",
    "filename_dataset = tf.data.Dataset.list_files(train_filenames)\n",
    "\n",
    "# 根据文件获取文件数据，把数据聚合在一起dataset返回\n",
    "raw_dataset = filename_dataset.interleave(\n",
    "    lambda filename: tf.data.TFRecordDataset(filename),\n",
    "    cycle_length = 3\n",
    ")\n",
    "\n",
    "# 定义解析方法\n",
    "def parse_serialized_example(serialized_example):\n",
    "    example = tf.io.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={\n",
    "            \"image\": tf.io.FixedLenFeature([1], tf.string),\n",
    "            \"label\": tf.io.FixedLenFeature([1], tf.int64)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    image = tf.io.decode_raw(example[\"image\"], tf.uint8)\n",
    "    image = tf.reshape(image, shape=[784])\n",
    "    label = example[\"label\"][0]\n",
    "    \n",
    "    return image, label\n",
    "\n",
    "# map 解析，元素，一一对应处理\n",
    "dataset = raw_dataset.map(\n",
    "    parse_serialized_example,\n",
    "    num_parallel_calls=2)\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "# shuffle 打乱顺序\n",
    "dataset = dataset.shuffle(10000)\n",
    "\n",
    "# 数据重复epoch\n",
    "dataset = dataset.repeat(epochs) # epochs = None ， 重复无线次\n",
    "\n",
    "\n",
    "# 使用map 做数据增强，元素一一对应处理\n",
    "'''\n",
    "def enhance_data(image):\n",
    "    \n",
    "#    图片处理过程, resize固定大小, 翻转，裁剪，变色，等等\n",
    "    \n",
    "    return image\n",
    "\n",
    "dataset = dataset.map(enhance_data)\n",
    "'''\n",
    "# batch 以batch_size 返回\n",
    "dataset = dataset.batch(batch_size=batch_size)\n",
    "\n",
    "# 生成迭代器\n",
    "dataset_iter = dataset.make_one_shot_iterator()\n",
    "\n",
    "# 获取一个元素\n",
    "images, labels = dataset_iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for _ in range(10):\n",
    "        images_v, label_v = sess.run([images, labels])\n",
    "        \n",
    "        print(\"images: {}, labels: {}\".format(images_v.shape, label_v.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. initializable_iterator动态初始化数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-730e361f1b02>:25: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n",
      "name: age, type: <class 'numpy.int64'>, value: 23\n",
      "name: sex, type: <class 'bytes'>, value: b'men'\n",
      "name: salary, type: <class 'numpy.float32'>, value: 2334.800048828125\n",
      "name: age, type: <class 'numpy.int64'>, value: 26\n",
      "name: sex, type: <class 'bytes'>, value: b'men'\n",
      "name: salary, type: <class 'numpy.float32'>, value: 2334.800048828125\n",
      "name: age, type: <class 'numpy.int64'>, value: 24\n",
      "name: sex, type: <class 'bytes'>, value: b'men'\n",
      "name: salary, type: <class 'numpy.float32'>, value: 5433.7001953125\n",
      "name: age, type: <class 'numpy.int64'>, value: 27\n",
      "name: sex, type: <class 'bytes'>, value: b'men'\n",
      "name: salary, type: <class 'numpy.float32'>, value: 5433.7001953125\n",
      "name: age, type: <class 'numpy.int64'>, value: 25\n",
      "name: sex, type: <class 'bytes'>, value: b'women'\n",
      "name: salary, type: <class 'numpy.float32'>, value: 7334.7998046875\n",
      "name: age, type: <class 'numpy.int64'>, value: 28\n",
      "name: sex, type: <class 'bytes'>, value: b'women'\n",
      "name: salary, type: <class 'numpy.float32'>, value: 7334.7998046875\n"
     ]
    }
   ],
   "source": [
    "# placeholder\n",
    "filenames_placeholder = tf.placeholder(tf.string)   ################不一样##################\n",
    "\n",
    "# 文件列表dataset\n",
    "filename_dataset = tf.data.Dataset.list_files(filenames_placeholder)\n",
    "\n",
    "# 根据文件名，读取数据，聚合成一个新的dataset\n",
    "csv_dataset = filename_dataset.interleave(\n",
    "    lambda filename: tf.data.TextLineDataset(filename).skip(1)\n",
    ")\n",
    "\n",
    "# 解析\n",
    "def parse_csv(line):\n",
    "    default_fields = [tf.constant(0, dtype=tf.int64), tf.constant(\"gg\", dtype=tf.string), tf.constant(0.0, dtype=tf.float32)]\n",
    "    parse_line = tf.io.decode_csv(line, default_fields) # 返回的都是list结构，元素都是tensor\n",
    "    age = parse_line[0]\n",
    "    sex = parse_line[1]\n",
    "    salary = parse_line[2]\n",
    "    return age, sex, salary\n",
    "\n",
    "# 对dataset元素处理，不改变元素数量\n",
    "dataset = csv_dataset.map(parse_csv)\n",
    "\n",
    "# 定义迭代器\n",
    "dataset_iter = dataset.make_initializable_iterator()\n",
    "\n",
    "# 获取一个迭代元素\n",
    "age, sex, salary = dataset_iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(dataset_iter.initializer, feed_dict = {filenames_placeholder: [csv_path01, csv_path02]}) ################不一样##################\n",
    "    \n",
    "    for _ in range(6):\n",
    "        age_value, sex_value, salary_value = sess.run([age, sex, salary])\n",
    "        print(\"name: age, type: {}, value: {}\".format(type(age_value), age_value))\n",
    "        print(\"name: sex, type: {}, value: {}\".format(type(sex_value), sex_value))\n",
    "        print(\"name: salary, type: {}, value: {}\".format(type(salary_value), salary_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
