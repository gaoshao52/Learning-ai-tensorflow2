{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目录\n",
    "## 1. 导入模块\n",
    "## 2. `tf.GradientTape`的基本用法\n",
    "  - ### 2.1 初次使用\n",
    "    - `tf.GradientTape`\n",
    "\t- `gradient`\n",
    "\t\n",
    "  - ### 2.2 打开`persistent=True`\n",
    "    - `tf.GradientTape(persistent=True)`\n",
    "\t- `gradient`\n",
    "\t\n",
    "  - ### 2.3 参数一起求导\n",
    "  - ### 2.4 常量必需要申明\n",
    "    - `watch`\n",
    "\t\n",
    "  - ### 2.5 两个函数对一个参数求导\n",
    "  - ### 2.6 二次求导\n",
    "  - ### 2.7 求凸函数的极值\n",
    "    - `apply_gradients`\n",
    "  \n",
    "## 3. 应用：神经网络优化参数\n",
    "  - ### 3.1 获取加利福尼亚数据\n",
    "  - ### 3.2 封装的优化算法训练\n",
    "  - ### 3.3 自定义优化算法训练\n",
    "    - `keras.metrics.MeanSquaredError`\n",
    "\t- `reset_states`\n",
    "\t- `keras.optimizers.SGD`\n",
    "\t- `np.random.randint`\n",
    "\t- `tf.GradientTape`\n",
    "\t- `keras.losses.mean_squared_error`\n",
    "\t- `gradient`\n",
    "\t- `apply_gradients`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy 1.18.1\n",
      "pandas 0.25.3\n",
      "matplotlib 3.1.2\n",
      "sklearn 0.22.1\n",
      "tensorflow_core.python.keras.api._v2.keras 2.2.4-tf\n",
      "tensorflow 2.1.0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "for module in [np, pd, mpl, sklearn, keras, tf]:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. `tf.GradientTape`的基本用法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x1, x2):\n",
    "    return (x1 + 5)*(x2**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - ### 2.1 初次使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(9.0, shape=(), dtype=float32)\n",
      "GradientTape.gradient can only be called once on non-persistent tapes.\n"
     ]
    }
   ],
   "source": [
    "v1 = tf.Variable(2.0)\n",
    "v2 = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = g(v1, v2)\n",
    "\n",
    "dz_v1 = tape.gradient(z, v1)\n",
    "print(dz_v1)\n",
    "\n",
    "try:\n",
    "    dz_v2 = tape.gradient(z, v2) # tape 默认只能调用一次\n",
    "    print(dz_v2)\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - ### 2.2 打开`persistent=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(9.0, shape=(), dtype=float32)\n",
      "tf.Tensor(42.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "v1 = tf.Variable(2.0)\n",
    "v2 = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape: # persistent=True  tape 可以调用很多次\n",
    "    z = g(v1, v2)\n",
    "\n",
    "dz_v1 = tape.gradient(z, v1)\n",
    "print(dz_v1)\n",
    "\n",
    "dz_v2 = tape.gradient(z, v2)\n",
    "print(dz_v2)\n",
    "\n",
    "del tape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - ### 2.3 参数一起求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=float32, numpy=9.0>, <tf.Tensor: shape=(), dtype=float32, numpy=42.0>]\n"
     ]
    }
   ],
   "source": [
    "v1 = tf.Variable(2.0)\n",
    "v2 = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape: \n",
    "    z = g(v1, v2)\n",
    "\n",
    "dz_v1v2 = tape.gradient(z, [v1, v2]) # 参数可以一起求导\n",
    "print(dz_v1v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - ### 2.4 常量必需要申明"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None]\n"
     ]
    }
   ],
   "source": [
    "v1 = tf.constant(2.0)\n",
    "v2 = tf.constant(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape: \n",
    "    z = g(v1, v2)\n",
    "\n",
    "dz_v1v2 = tape.gradient(z, [v1, v2]) \n",
    "print(dz_v1v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=float32, numpy=9.0>, <tf.Tensor: shape=(), dtype=float32, numpy=42.0>]\n"
     ]
    }
   ],
   "source": [
    "v1 = tf.constant(2.0)\n",
    "v2 = tf.constant(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape: \n",
    "    tape.watch(v1)\n",
    "    tape.watch(v2)\n",
    "    z = g(v1, v2)\n",
    "\n",
    "dz_v1v2 = tape.gradient(z, [v1, v2]) \n",
    "print(dz_v1v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - ### 2.5 两个函数对一个参数求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=77.99999>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable(5.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z1 = x*3\n",
    "    z2 = x**3\n",
    "\n",
    "tape.gradient([z1, z2], x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - ### 2.6 二次求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[None, <tf.Tensor: shape=(), dtype=float32, numpy=6.0>], [<tf.Tensor: shape=(), dtype=float32, numpy=6.0>, <tf.Tensor: shape=(), dtype=float32, numpy=14.0>]]\n"
     ]
    }
   ],
   "source": [
    "v1 = tf.Variable(2.0)\n",
    "v2 = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as outer_tape:\n",
    "    with tf.GradientTape(persistent=True) as inner_tape:\n",
    "        z = g(v1, v2)\n",
    "    inner_grads = inner_tape.gradient(z, [v1, v2])\n",
    "outer_grads = [outer_tape.gradient(inner_grad, [v1, v2])for inner_grad in inner_grads]\n",
    "\n",
    "del outer_tape\n",
    "del inner_tape\n",
    "\n",
    "print(outer_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - ### 2.7 求凸函数的极值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.33333087>\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return 3. * x **2 + 2. * x - 1\n",
    "\n",
    "#  初始化 x\n",
    "x = tf.Variable(0.0)\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "# 梯度下降\n",
    "for _ in range(5000):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y = f(x)\n",
    "    dy_dx = tape.gradient(y, x)\n",
    "    x.assign_sub(learning_rate * dy_dx)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.33333087>\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return 3. * x **2 + 2. * x - 1\n",
    "\n",
    "#  初始化 x\n",
    "x = tf.Variable(0.0)\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "\n",
    "# 梯度下降\n",
    "for _ in range(5000):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y = f(x)\n",
    "    dy_dx = tape.gradient(y, x)\n",
    "    optimizer.apply_gradients([(dy_dx, x)])\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 应用：神经网络优化参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - ### 3.1 获取加利福尼亚数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11610, 8) (11610,)\n",
      "(3870, 8) (3870,)\n",
      "(5160, 8) (5160,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "x_train_all, x_test, y_train_all, y_test = train_test_split(housing.data, housing.target, random_state=12234)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train_all, y_train_all, random_state=1234)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_valid_scaled = scaler.transform(x_valid)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "print(x_train_scaled.shape, y_train.shape)\n",
    "print(x_valid_scaled.shape, y_valid.shape)\n",
    "print(x_test_scaled.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - ### 3.2 封装的优化算法训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "11610/11610 [==============================] - 1s 89us/sample - loss: 1.6360 - mse: 1.6360 - val_loss: 1.9553 - val_mse: 1.9553\n",
      "Epoch 2/100\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.7432 - mse: 0.7432 - val_loss: 0.7036 - val_mse: 0.7036\n",
      "Epoch 3/100\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.6359 - mse: 0.6359 - val_loss: 0.6331 - val_mse: 0.6331\n",
      "Epoch 4/100\n",
      "11610/11610 [==============================] - 1s 67us/sample - loss: 0.5844 - mse: 0.5844 - val_loss: 0.5750 - val_mse: 0.5750\n",
      "Epoch 5/100\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.5485 - mse: 0.5485 - val_loss: 0.5451 - val_mse: 0.5451\n",
      "Epoch 6/100\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.5214 - mse: 0.5214 - val_loss: 0.5154 - val_mse: 0.5154\n",
      "Epoch 7/100\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.4992 - mse: 0.4992 - val_loss: 0.4949 - val_mse: 0.4949\n",
      "Epoch 8/100\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.4813 - mse: 0.4813 - val_loss: 0.4730 - val_mse: 0.4730\n",
      "Epoch 9/100\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.4655 - mse: 0.4655 - val_loss: 0.4519 - val_mse: 0.4519\n",
      "Epoch 10/100\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.4539 - mse: 0.4539 - val_loss: 0.4602 - val_mse: 0.4602\n",
      "Epoch 11/100\n",
      "11610/11610 [==============================] - 1s 67us/sample - loss: 0.4432 - mse: 0.4432 - val_loss: 0.4559 - val_mse: 0.4559\n",
      "Epoch 12/100\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.4343 - mse: 0.4343 - val_loss: 0.4361 - val_mse: 0.4361\n",
      "Epoch 13/100\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.4258 - mse: 0.4258 - val_loss: 0.4186 - val_mse: 0.4186\n",
      "Epoch 14/100\n",
      "11610/11610 [==============================] - 1s 68us/sample - loss: 0.4195 - mse: 0.4195 - val_loss: 0.4218 - val_mse: 0.4218\n",
      "Epoch 15/100\n",
      "11610/11610 [==============================] - 1s 68us/sample - loss: 0.4137 - mse: 0.4137 - val_loss: 0.4070 - val_mse: 0.4070\n",
      "Epoch 16/100\n",
      "11610/11610 [==============================] - 1s 68us/sample - loss: 0.4083 - mse: 0.4083 - val_loss: 0.4053 - val_mse: 0.4053\n",
      "Epoch 17/100\n",
      "11610/11610 [==============================] - 1s 69us/sample - loss: 0.4039 - mse: 0.4039 - val_loss: 0.4057 - val_mse: 0.4057\n",
      "Epoch 18/100\n",
      "11610/11610 [==============================] - 1s 70us/sample - loss: 0.3993 - mse: 0.3993 - val_loss: 0.4034 - val_mse: 0.4034\n",
      "Epoch 19/100\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.3955 - mse: 0.3955 - val_loss: 0.3908 - val_mse: 0.3908\n",
      "Epoch 20/100\n",
      "11610/11610 [==============================] - 1s 68us/sample - loss: 0.3917 - mse: 0.3917 - val_loss: 0.3934 - val_mse: 0.3934\n",
      "Epoch 21/100\n",
      "11610/11610 [==============================] - 1s 67us/sample - loss: 0.3887 - mse: 0.3887 - val_loss: 0.3877 - val_mse: 0.3877\n",
      "Epoch 22/100\n",
      "11610/11610 [==============================] - 1s 67us/sample - loss: 0.3855 - mse: 0.3855 - val_loss: 0.3865 - val_mse: 0.3865\n",
      "Epoch 23/100\n",
      "11610/11610 [==============================] - 1s 67us/sample - loss: 0.3829 - mse: 0.3829 - val_loss: 0.3811 - val_mse: 0.3811\n",
      "Epoch 24/100\n",
      "11610/11610 [==============================] - 1s 67us/sample - loss: 0.3805 - mse: 0.3805 - val_loss: 0.3816 - val_mse: 0.3816\n",
      "Epoch 25/100\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.3780 - mse: 0.3780 - val_loss: 0.3851 - val_mse: 0.3851\n",
      "Epoch 26/100\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.3760 - mse: 0.3760 - val_loss: 0.3716 - val_mse: 0.3716\n",
      "Epoch 27/100\n",
      "11610/11610 [==============================] - 1s 67us/sample - loss: 0.3738 - mse: 0.3738 - val_loss: 0.3683 - val_mse: 0.3683\n",
      "Epoch 28/100\n",
      "11610/11610 [==============================] - 1s 67us/sample - loss: 0.3722 - mse: 0.3722 - val_loss: 0.3823 - val_mse: 0.3823\n",
      "Epoch 29/100\n",
      "11610/11610 [==============================] - 1s 68us/sample - loss: 0.3703 - mse: 0.3703 - val_loss: 0.3683 - val_mse: 0.3683\n",
      "Epoch 30/100\n",
      "11610/11610 [==============================] - 1s 67us/sample - loss: 0.3685 - mse: 0.3685 - val_loss: 0.3690 - val_mse: 0.3690\n",
      "Epoch 31/100\n",
      "11610/11610 [==============================] - 1s 68us/sample - loss: 0.3671 - mse: 0.3671 - val_loss: 0.3691 - val_mse: 0.3691\n",
      "Epoch 32/100\n",
      "11610/11610 [==============================] - 1s 67us/sample - loss: 0.3653 - mse: 0.3653 - val_loss: 0.3611 - val_mse: 0.3611\n",
      "Epoch 33/100\n",
      "11610/11610 [==============================] - 1s 67us/sample - loss: 0.3640 - mse: 0.3640 - val_loss: 0.3694 - val_mse: 0.3694\n",
      "Epoch 34/100\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.3626 - mse: 0.3626 - val_loss: 0.3571 - val_mse: 0.3571\n",
      "Epoch 35/100\n",
      "11610/11610 [==============================] - 1s 65us/sample - loss: 0.3611 - mse: 0.3611 - val_loss: 0.3654 - val_mse: 0.3654\n",
      "Epoch 36/100\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.3596 - mse: 0.3596 - val_loss: 0.3656 - val_mse: 0.3656\n",
      "Epoch 37/100\n",
      "11610/11610 [==============================] - 1s 68us/sample - loss: 0.3587 - mse: 0.3587 - val_loss: 0.3621 - val_mse: 0.3621\n",
      "Epoch 38/100\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.3572 - mse: 0.3572 - val_loss: 0.3638 - val_mse: 0.3638\n",
      "Epoch 39/100\n",
      "11610/11610 [==============================] - 1s 70us/sample - loss: 0.3561 - mse: 0.3561 - val_loss: 0.3541 - val_mse: 0.3541\n",
      "Epoch 40/100\n",
      "11610/11610 [==============================] - 1s 70us/sample - loss: 0.3550 - mse: 0.3550 - val_loss: 0.3593 - val_mse: 0.3593\n",
      "Epoch 41/100\n",
      "11610/11610 [==============================] - 1s 68us/sample - loss: 0.3538 - mse: 0.3538 - val_loss: 0.3575 - val_mse: 0.3575\n",
      "Epoch 42/100\n",
      "11610/11610 [==============================] - 1s 70us/sample - loss: 0.3528 - mse: 0.3528 - val_loss: 0.3574 - val_mse: 0.3574\n",
      "Epoch 43/100\n",
      "11610/11610 [==============================] - 1s 71us/sample - loss: 0.3514 - mse: 0.3514 - val_loss: 0.3470 - val_mse: 0.3470\n",
      "Epoch 44/100\n",
      "11610/11610 [==============================] - 1s 70us/sample - loss: 0.3512 - mse: 0.3512 - val_loss: 0.3459 - val_mse: 0.3459\n",
      "Epoch 45/100\n",
      "11610/11610 [==============================] - 1s 69us/sample - loss: 0.3500 - mse: 0.3500 - val_loss: 0.3623 - val_mse: 0.3623\n",
      "Epoch 46/100\n",
      "11610/11610 [==============================] - 1s 69us/sample - loss: 0.3490 - mse: 0.3490 - val_loss: 0.3430 - val_mse: 0.3430\n",
      "Epoch 47/100\n",
      "11610/11610 [==============================] - 1s 70us/sample - loss: 0.3480 - mse: 0.3480 - val_loss: 0.3533 - val_mse: 0.3533\n",
      "Epoch 48/100\n",
      "11610/11610 [==============================] - 1s 70us/sample - loss: 0.3470 - mse: 0.3470 - val_loss: 0.3511 - val_mse: 0.3511\n",
      "Epoch 49/100\n",
      "11610/11610 [==============================] - 1s 69us/sample - loss: 0.3458 - mse: 0.3458 - val_loss: 0.3417 - val_mse: 0.3417\n",
      "Epoch 50/100\n",
      "11610/11610 [==============================] - 1s 69us/sample - loss: 0.3459 - mse: 0.3459 - val_loss: 0.3671 - val_mse: 0.3671\n",
      "Epoch 51/100\n",
      "11610/11610 [==============================] - 1s 70us/sample - loss: 0.3446 - mse: 0.3446 - val_loss: 0.3414 - val_mse: 0.3414\n",
      "Epoch 52/100\n",
      "11610/11610 [==============================] - 1s 70us/sample - loss: 0.3443 - mse: 0.3443 - val_loss: 0.3562 - val_mse: 0.3562\n",
      "Epoch 53/100\n",
      "11610/11610 [==============================] - 1s 70us/sample - loss: 0.3430 - mse: 0.3430 - val_loss: 0.3453 - val_mse: 0.3453\n",
      "Epoch 54/100\n",
      "11610/11610 [==============================] - 1s 70us/sample - loss: 0.3420 - mse: 0.3420 - val_loss: 0.3443 - val_mse: 0.3443\n",
      "Epoch 55/100\n",
      "11610/11610 [==============================] - 1s 71us/sample - loss: 0.3412 - mse: 0.3412 - val_loss: 0.3378 - val_mse: 0.3378\n",
      "Epoch 56/100\n",
      "11610/11610 [==============================] - 1s 72us/sample - loss: 0.3405 - mse: 0.3405 - val_loss: 0.3458 - val_mse: 0.3458\n",
      "Epoch 57/100\n",
      "11610/11610 [==============================] - 1s 69us/sample - loss: 0.3394 - mse: 0.3394 - val_loss: 0.3411 - val_mse: 0.3411\n",
      "Epoch 58/100\n",
      "11610/11610 [==============================] - 1s 67us/sample - loss: 0.3390 - mse: 0.3390 - val_loss: 0.3360 - val_mse: 0.3360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "11610/11610 [==============================] - 1s 68us/sample - loss: 0.3384 - mse: 0.3384 - val_loss: 0.3428 - val_mse: 0.3428\n",
      "Epoch 60/100\n",
      "11610/11610 [==============================] - 1s 69us/sample - loss: 0.3373 - mse: 0.3373 - val_loss: 0.3400 - val_mse: 0.3400\n",
      "Epoch 61/100\n",
      "11610/11610 [==============================] - 1s 69us/sample - loss: 0.3364 - mse: 0.3364 - val_loss: 0.3406 - val_mse: 0.3406\n",
      "Epoch 62/100\n",
      "11610/11610 [==============================] - 1s 68us/sample - loss: 0.3359 - mse: 0.3359 - val_loss: 0.3463 - val_mse: 0.3463\n",
      "Epoch 63/100\n",
      "11610/11610 [==============================] - 1s 69us/sample - loss: 0.3355 - mse: 0.3355 - val_loss: 0.3399 - val_mse: 0.3399\n",
      "Epoch 64/100\n",
      "11610/11610 [==============================] - 1s 68us/sample - loss: 0.3347 - mse: 0.3347 - val_loss: 0.3349 - val_mse: 0.3349\n",
      "Epoch 65/100\n",
      "11610/11610 [==============================] - 1s 68us/sample - loss: 0.3343 - mse: 0.3343 - val_loss: 0.3518 - val_mse: 0.3518\n",
      "Epoch 66/100\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.3339 - mse: 0.3339 - val_loss: 0.3297 - val_mse: 0.3297\n",
      "Epoch 67/100\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.3333 - mse: 0.3333 - val_loss: 0.3839 - val_mse: 0.3839\n",
      "Epoch 68/100\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.3340 - mse: 0.3340 - val_loss: 0.3442 - val_mse: 0.3442\n",
      "Epoch 69/100\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.3336 - mse: 0.3336 - val_loss: 0.4573 - val_mse: 0.4573\n",
      "Epoch 70/100\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.3356 - mse: 0.3356 - val_loss: 0.3869 - val_mse: 0.3869\n",
      "Epoch 71/100\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.3357 - mse: 0.3357 - val_loss: 0.6309 - val_mse: 0.6309\n",
      "Epoch 72/100\n",
      "11610/11610 [==============================] - 1s 67us/sample - loss: 0.3407 - mse: 0.3407 - val_loss: 0.5859 - val_mse: 0.5859\n",
      "Epoch 73/100\n",
      "11610/11610 [==============================] - 1s 70us/sample - loss: 0.3466 - mse: 0.3466 - val_loss: 0.5237 - val_mse: 0.5237\n",
      "Epoch 74/100\n",
      "11610/11610 [==============================] - 1s 69us/sample - loss: 0.3379 - mse: 0.3379 - val_loss: 0.3660 - val_mse: 0.3660\n",
      "Epoch 75/100\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.3329 - mse: 0.3329 - val_loss: 0.3857 - val_mse: 0.3857\n",
      "Epoch 76/100\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.3302 - mse: 0.3302 - val_loss: 0.3261 - val_mse: 0.3261\n",
      "Epoch 77/100\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.3276 - mse: 0.3276 - val_loss: 0.3377 - val_mse: 0.3377\n",
      "Epoch 78/100\n",
      "11610/11610 [==============================] - 1s 67us/sample - loss: 0.3273 - mse: 0.3273 - val_loss: 0.3272 - val_mse: 0.3272\n",
      "Epoch 79/100\n",
      "11610/11610 [==============================] - 1s 70us/sample - loss: 0.3265 - mse: 0.3265 - val_loss: 0.3275 - val_mse: 0.3275\n",
      "Epoch 80/100\n",
      "11610/11610 [==============================] - 1s 70us/sample - loss: 0.3258 - mse: 0.3258 - val_loss: 0.3324 - val_mse: 0.3324\n",
      "Epoch 81/100\n",
      "11610/11610 [==============================] - 1s 69us/sample - loss: 0.3255 - mse: 0.3255 - val_loss: 0.3233 - val_mse: 0.3233\n",
      "Epoch 82/100\n",
      "11610/11610 [==============================] - 1s 69us/sample - loss: 0.3249 - mse: 0.3249 - val_loss: 0.3351 - val_mse: 0.3351\n",
      "Epoch 83/100\n",
      "11610/11610 [==============================] - 1s 69us/sample - loss: 0.3241 - mse: 0.3241 - val_loss: 0.3287 - val_mse: 0.3287\n",
      "Epoch 84/100\n",
      "11610/11610 [==============================] - 1s 69us/sample - loss: 0.3238 - mse: 0.3238 - val_loss: 0.3315 - val_mse: 0.3315\n",
      "Epoch 85/100\n",
      "11610/11610 [==============================] - 1s 70us/sample - loss: 0.3233 - mse: 0.3233 - val_loss: 0.3229 - val_mse: 0.3229\n",
      "Epoch 86/100\n",
      "11610/11610 [==============================] - 1s 71us/sample - loss: 0.3230 - mse: 0.3230 - val_loss: 0.3440 - val_mse: 0.3440\n",
      "Epoch 87/100\n",
      "11610/11610 [==============================] - 1s 70us/sample - loss: 0.3225 - mse: 0.3225 - val_loss: 0.3211 - val_mse: 0.3211\n",
      "Epoch 88/100\n",
      "11610/11610 [==============================] - 1s 69us/sample - loss: 0.3217 - mse: 0.3217 - val_loss: 0.3344 - val_mse: 0.3344\n",
      "Epoch 89/100\n",
      "11610/11610 [==============================] - 1s 70us/sample - loss: 0.3214 - mse: 0.3214 - val_loss: 0.3214 - val_mse: 0.3214\n",
      "Epoch 90/100\n",
      "11610/11610 [==============================] - 1s 70us/sample - loss: 0.3205 - mse: 0.3205 - val_loss: 0.3308 - val_mse: 0.3308\n",
      "Epoch 91/100\n",
      "11610/11610 [==============================] - 1s 70us/sample - loss: 0.3202 - mse: 0.3202 - val_loss: 0.3205 - val_mse: 0.3205\n",
      "Epoch 92/100\n",
      "11610/11610 [==============================] - 1s 69us/sample - loss: 0.3197 - mse: 0.3197 - val_loss: 0.3267 - val_mse: 0.3267\n",
      "Epoch 93/100\n",
      "11610/11610 [==============================] - 1s 70us/sample - loss: 0.3192 - mse: 0.3192 - val_loss: 0.3305 - val_mse: 0.3305\n",
      "Epoch 94/100\n",
      "11610/11610 [==============================] - 1s 70us/sample - loss: 0.3188 - mse: 0.3188 - val_loss: 0.3219 - val_mse: 0.3219\n",
      "Epoch 95/100\n",
      "11610/11610 [==============================] - 1s 70us/sample - loss: 0.3183 - mse: 0.3183 - val_loss: 0.3285 - val_mse: 0.3285\n",
      "Epoch 96/100\n",
      "11610/11610 [==============================] - 1s 70us/sample - loss: 0.3177 - mse: 0.3177 - val_loss: 0.3257 - val_mse: 0.3257\n",
      "Epoch 97/100\n",
      "11610/11610 [==============================] - 1s 70us/sample - loss: 0.3173 - mse: 0.3173 - val_loss: 0.3194 - val_mse: 0.3194\n",
      "Epoch 98/100\n",
      "11610/11610 [==============================] - 1s 67us/sample - loss: 0.3176 - mse: 0.3176 - val_loss: 0.3249 - val_mse: 0.3249\n",
      "Epoch 99/100\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.3167 - mse: 0.3167 - val_loss: 0.3196 - val_mse: 0.3196\n",
      "Epoch 100/100\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.3158 - mse: 0.3158 - val_loss: 0.3342 - val_mse: 0.3342\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(100, input_shape=[8], activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.SGD(1e-3), metrics=[\"mse\"])\n",
    "\n",
    "history = model.fit(x_train_scaled, y_train, validation_data=(x_valid_scaled, y_valid), epochs=100, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - ### 3.3 自定义优化算法训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "小知识：metric 的用法是用于累加求度量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(49.0, shape=(), dtype=float32)\n",
      "tf.Tensor(25.0, shape=(), dtype=float32)\n",
      "tf.Tensor(25.0, shape=(), dtype=float32)\n",
      "tf.Tensor(49.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "metric = keras.metrics.MeanSquaredError()\n",
    "\n",
    "print(metric([2.0], [9.0]))\n",
    "print(metric([1.0], [0.0]))\n",
    "print(metric.result())\n",
    "\n",
    "metric.reset_states()  # 重新开始\n",
    "print(metric([2.0], [9.0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dense_12 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "[train] epoch: 0, mse: 2.0710\t[valid] mes: 2.2600\n",
      "[train] epoch: 1, mse: 0.7963\t[valid] mes: 0.8227\n",
      "[train] epoch: 2, mse: 0.6594\t[valid] mes: 0.9071\n",
      "[train] epoch: 3, mse: 0.5974\t[valid] mes: 0.9768\n",
      "[train] epoch: 4, mse: 0.5948\t[valid] mes: 0.8132\n",
      "[train] epoch: 5, mse: 0.5529\t[valid] mes: 0.5765\n",
      "[train] epoch: 6, mse: 0.5161\t[valid] mes: 0.5206\n",
      "[train] epoch: 7, mse: 0.5092\t[valid] mes: 0.4945\n",
      "[train] epoch: 8, mse: 0.4716\t[valid] mes: 0.4976\n",
      "[train] epoch: 9, mse: 0.4598\t[valid] mes: 0.5234\n",
      "[train] epoch: 10, mse: 0.4594\t[valid] mes: 0.4487\n",
      "[train] epoch: 11, mse: 0.4455\t[valid] mes: 0.4482\n",
      "[train] epoch: 12, mse: 0.4274\t[valid] mes: 0.4462\n",
      "[train] epoch: 13, mse: 0.4203\t[valid] mes: 0.4369\n",
      "[train] epoch: 14, mse: 0.4196\t[valid] mes: 0.4182\n",
      "[train] epoch: 15, mse: 0.4029\t[valid] mes: 0.4175\n",
      "[train] epoch: 16, mse: 0.4202\t[valid] mes: 0.4279\n",
      "[train] epoch: 17, mse: 0.4285\t[valid] mes: 0.4311\n",
      "[train] epoch: 18, mse: 0.4036\t[valid] mes: 0.4049\n",
      "[train] epoch: 19, mse: 0.3877\t[valid] mes: 0.3987\n",
      "[train] epoch: 20, mse: 0.4058\t[valid] mes: 0.4188\n",
      "[train] epoch: 21, mse: 0.3958\t[valid] mes: 0.4038\n",
      "[train] epoch: 22, mse: 0.3758\t[valid] mes: 0.4049\n",
      "[train] epoch: 23, mse: 0.3873\t[valid] mes: 0.4040\n",
      "[train] epoch: 24, mse: 0.3782\t[valid] mes: 0.4034\n",
      "[train] epoch: 25, mse: 0.3888\t[valid] mes: 0.3992\n",
      "[train] epoch: 26, mse: 0.3759\t[valid] mes: 0.4008\n",
      "[train] epoch: 27, mse: 0.3817\t[valid] mes: 0.3842\n",
      "[train] epoch: 28, mse: 0.3687\t[valid] mes: 0.3911\n",
      "[train] epoch: 29, mse: 0.3801\t[valid] mes: 0.3832\n",
      "[train] epoch: 30, mse: 0.3695\t[valid] mes: 0.3871\n",
      "[train] epoch: 31, mse: 0.3632\t[valid] mes: 0.3810\n",
      "[train] epoch: 32, mse: 0.3560\t[valid] mes: 0.3791\n",
      "[train] epoch: 33, mse: 0.3585\t[valid] mes: 0.3809\n",
      "[train] epoch: 34, mse: 0.3721\t[valid] mes: 0.3822\n",
      "[train] epoch: 35, mse: 0.3658\t[valid] mes: 0.3733\n",
      "[train] epoch: 36, mse: 0.3595\t[valid] mes: 0.3759\n",
      "[train] epoch: 37, mse: 0.3702\t[valid] mes: 0.3854\n",
      "[train] epoch: 38, mse: 0.3706\t[valid] mes: 0.3933\n",
      "[train] epoch: 39, mse: 0.3597\t[valid] mes: 0.3659\n",
      "[train] epoch: 40, mse: 0.3683\t[valid] mes: 0.3739\n",
      "[train] epoch: 41, mse: 0.3424\t[valid] mes: 0.3718\n",
      "[train] epoch: 42, mse: 0.3547\t[valid] mes: 0.3773\n",
      "[train] epoch: 43, mse: 0.3612\t[valid] mes: 0.3697\n",
      "[train] epoch: 44, mse: 0.3591\t[valid] mes: 0.3762\n",
      "[train] epoch: 45, mse: 0.3565\t[valid] mes: 0.3767\n",
      "[train] epoch: 46, mse: 0.3520\t[valid] mes: 0.3672\n",
      "[train] epoch: 47, mse: 0.3470\t[valid] mes: 0.3560\n",
      "[train] epoch: 48, mse: 0.3469\t[valid] mes: 0.3856\n",
      "[train] epoch: 49, mse: 0.3515\t[valid] mes: 0.3914\n",
      "[train] epoch: 50, mse: 0.3480\t[valid] mes: 0.3919\n",
      "[train] epoch: 51, mse: 0.3375\t[valid] mes: 0.3524\n",
      "[train] epoch: 52, mse: 0.3546\t[valid] mes: 0.3637\n",
      "[train] epoch: 53, mse: 0.3350\t[valid] mes: 0.3578\n",
      "[train] epoch: 54, mse: 0.3417\t[valid] mes: 0.3545\n",
      "[train] epoch: 55, mse: 0.3502\t[valid] mes: 0.3566\n",
      "[train] epoch: 56, mse: 0.3489\t[valid] mes: 0.3634\n",
      "[train] epoch: 57, mse: 0.3480\t[valid] mes: 0.3468\n",
      "[train] epoch: 58, mse: 0.3325\t[valid] mes: 0.3498\n",
      "[train] epoch: 59, mse: 0.3282\t[valid] mes: 0.3595\n",
      "[train] epoch: 60, mse: 0.3549\t[valid] mes: 0.3824\n",
      "[train] epoch: 61, mse: 0.3550\t[valid] mes: 0.3583\n",
      "[train] epoch: 62, mse: 0.3345\t[valid] mes: 0.3446\n",
      "[train] epoch: 63, mse: 0.3399\t[valid] mes: 0.3570\n",
      "[train] epoch: 64, mse: 0.3334\t[valid] mes: 0.3523\n",
      "[train] epoch: 65, mse: 0.3339\t[valid] mes: 0.3635\n",
      "[train] epoch: 66, mse: 0.3404\t[valid] mes: 0.3767\n",
      "[train] epoch: 67, mse: 0.3290\t[valid] mes: 0.3920\n",
      "[train] epoch: 68, mse: 0.3311\t[valid] mes: 0.3906\n",
      "[train] epoch: 69, mse: 0.3300\t[valid] mes: 0.3387\n",
      "[train] epoch: 70, mse: 0.3324\t[valid] mes: 0.3428\n",
      "[train] epoch: 71, mse: 0.3317\t[valid] mes: 0.3442\n",
      "[train] epoch: 72, mse: 0.3388\t[valid] mes: 0.3438\n",
      "[train] epoch: 73, mse: 0.3297\t[valid] mes: 0.3411\n",
      "[train] epoch: 74, mse: 0.3296\t[valid] mes: 0.3474\n",
      "[train] epoch: 75, mse: 0.2989\t[valid] mes: 0.3496\n",
      "[train] epoch: 76, mse: 0.3333\t[valid] mes: 0.3531\n",
      "[train] epoch: 77, mse: 0.3320\t[valid] mes: 0.3381\n",
      "[train] epoch: 78, mse: 0.3169\t[valid] mes: 0.3452\n",
      "[train] epoch: 79, mse: 0.3363\t[valid] mes: 0.3717\n",
      "[train] epoch: 80, mse: 0.3261\t[valid] mes: 0.3351\n",
      "[train] epoch: 81, mse: 0.3284\t[valid] mes: 0.3382\n",
      "[train] epoch: 82, mse: 0.3411\t[valid] mes: 0.3505\n",
      "[train] epoch: 83, mse: 0.3192\t[valid] mes: 0.3309\n",
      "[train] epoch: 84, mse: 0.3231\t[valid] mes: 0.3469\n",
      "[train] epoch: 85, mse: 0.3339\t[valid] mes: 0.3479\n",
      "[train] epoch: 86, mse: 0.3316\t[valid] mes: 0.3384\n",
      "[train] epoch: 87, mse: 0.3183\t[valid] mes: 0.3305\n",
      "[train] epoch: 88, mse: 0.3217\t[valid] mes: 0.3388\n",
      "[train] epoch: 89, mse: 0.3268\t[valid] mes: 0.3342\n",
      "[train] epoch: 90, mse: 0.3408\t[valid] mes: 0.3532\n",
      "[train] epoch: 91, mse: 0.3172\t[valid] mes: 0.3421\n",
      "[train] epoch: 92, mse: 0.3163\t[valid] mes: 0.3532\n",
      "[train] epoch: 93, mse: 0.3217\t[valid] mes: 0.3543\n",
      "[train] epoch: 94, mse: 0.3292\t[valid] mes: 0.3278\n",
      "[train] epoch: 95, mse: 0.3215\t[valid] mes: 0.3319\n",
      "[train] epoch: 96, mse: 0.3174\t[valid] mes: 0.3410\n",
      "[train] epoch: 97, mse: 0.3190\t[valid] mes: 0.3465\n",
      "[train] epoch: 98, mse: 0.3182\t[valid] mes: 0.3257\n",
      "[train] epoch: 99, mse: 0.3322\t[valid] mes: 0.3297\n"
     ]
    }
   ],
   "source": [
    "# 定义模型\n",
    "model_gradient = keras.models.Sequential([\n",
    "    keras.layers.Dense(100, input_shape=[8], activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "steps_per_epoch = len(x_train) // batch_size\n",
    "\n",
    "metric = keras.metrics.MeanSquaredError()\n",
    "optimizer = keras.optimizers.SGD(1e-3)\n",
    "\n",
    "def generate_batch(x, y, batch_size=32):\n",
    "    idx = np.random.randint(0, len(x), size=batch_size)\n",
    "    return x[idx], y[idx]\n",
    "\n",
    "# 遍历epochs\n",
    "for epoch in range(epochs):\n",
    "    metric.reset_states()\n",
    "    \n",
    "    # 一个epoch遍历\n",
    "    for step in range(steps_per_epoch):\n",
    "        \n",
    "        # 拿数据\n",
    "        x_batch, y_batch = generate_batch(x_train_scaled, y_train)\n",
    "        \n",
    "        # 求梯度\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model_gradient(x_batch)\n",
    "            y_pred = tf.squeeze(y_pred, 1)\n",
    "            loss = keras.losses.mean_squared_error(y_batch, y_pred)\n",
    "            metric(y_batch, y_pred)\n",
    "        grads = tape.gradient(loss, model_gradient.trainable_variables)\n",
    "        \n",
    "        # 更新参数\n",
    "        grads_and_vars = zip(grads, model_gradient.trainable_variables)\n",
    "        optimizer.apply_gradients(grads_and_vars)\n",
    "        \n",
    "        print(\"\\r[train] epoch: {}, mse: {:.4f}\".format(epoch, metric.result().numpy()), end=\"\")\n",
    "        \n",
    "    # 验证\n",
    "    y_valid_pred = model_gradient(x_valid_scaled)\n",
    "    y_valid_pred = tf.squeeze(y_valid_pred, 1)\n",
    "    valid_loss = keras.losses.mean_squared_error(y_valid_pred, y_valid)\n",
    "    print(\"\\t[valid] mes: {:.4f}\".format(valid_loss))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
